{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Categorical\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v0'\n",
    "env = gym.make(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, action_num=2, hidden_size=256):\n",
    "        super(QNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, action_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        q_values = self.fc2(x)\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, data_names, buffer_size, mini_batch_size):\n",
    "        self.data_keys = data_names\n",
    "        self.data_dict = {}\n",
    "        self.buffer_size = buffer_size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Create a deque for each data type with set max length\n",
    "        for name in self.data_keys:\n",
    "            self.data_dict[name] = deque(maxlen=self.buffer_size)\n",
    "\n",
    "    def buffer_full(self):\n",
    "        return len(self.data_dict[self.data_keys[0]]) == self.buffer_size\n",
    "\n",
    "    def data_log(self, data_name, data):\n",
    "        # split tensor along batch into a list of individual datapoints\n",
    "        data = data.cpu().split(1)\n",
    "        # Extend the deque for data type, deque will handle popping old data to maintain buffer size\n",
    "        self.data_dict[data_name].extend(data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_size = len(self.data_dict[self.data_keys[0]])\n",
    "        batch_size = batch_size - batch_size % self.mini_batch_size\n",
    "\n",
    "        ids = np.random.permutation(batch_size)\n",
    "        ids = np.split(ids, batch_size // self.mini_batch_size)\n",
    "        for i in range(len(ids)):\n",
    "            batch_dict = {}\n",
    "            for name in self.data_keys:\n",
    "                c = [self.data_dict[name][j] for j in ids[i]]\n",
    "                batch_dict[name] = torch.cat(c)\n",
    "            batch_dict[\"batch_size\"] = len(ids[i])\n",
    "            yield batch_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dict[self.data_keys[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent():\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    observation = torch.FloatTensor(env.reset()).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not done:\n",
    "            q_values = q_net(observation)\n",
    "            action = q_values.argmax().cpu().item()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            \n",
    "            observation = torch.FloatTensor(observation).unsqueeze(0)\n",
    "            total_reward += reward\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_update():\n",
    "    for data_batch in replay_buffer:\n",
    "        next_q_values = q_net(data_batch[\"next_states\"]).detach()\n",
    "        q_values = q_net(data_batch[\"states\"])\n",
    "        \n",
    "        index_q_values = q_values.gather(1, data_batch[\"actions\"])\n",
    "        max_next_q_values = next_q_values.max(1)[0].unsqueeze(1)\n",
    "\n",
    "        expected_q_value = data_batch[\"rewards\"] + gamma * max_next_q_values * data_batch[\"masks\"]\n",
    "\n",
    "        q_loss = (index_q_values - expected_q_value).pow(2).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "gamma = 0.99\n",
    "buffer_size = 2000\n",
    "mini_batch_size = 32\n",
    "max_steps = 15000\n",
    "\n",
    "data_names = [\"states\", \"next_states\", \"actions\", \"rewards\", \"masks\"]\n",
    "\n",
    "q_net = QNet()\n",
    "optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "\n",
    "replay_buffer = ReplayBuffer(data_names, buffer_size, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = 0\n",
    "step = 0\n",
    "initial_epsilon = 1\n",
    "epsilon = initial_epsilon\n",
    "score_logger = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while step < max_steps:\n",
    "    observation =  torch.FloatTensor(env.reset()).unsqueeze(0)\n",
    "    done = False\n",
    "    \n",
    "    states = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    masks = []\n",
    "\n",
    "    while not done:\n",
    "        states.append(observation)\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            q_values = q_net(observation)\n",
    "            action = q_values.argmax().reshape(-1, 1)\n",
    "        else:\n",
    "            action = torch.LongTensor([env.action_space.sample()]).reshape(-1, 1)\n",
    "                \n",
    "        observation, reward, done, info = env.step(action.cpu().item())\n",
    "        \n",
    "        reward = torch.FloatTensor([reward]).unsqueeze(0)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        actions.append(action)\n",
    "        masks.append(torch.FloatTensor([1 - done]).unsqueeze(0))\n",
    "        \n",
    "        observation = torch.FloatTensor(observation).unsqueeze(0)\n",
    "        step += 1\n",
    "        \n",
    "    states.append(observation)\n",
    "    \n",
    "    replay_buffer.data_log(\"states\", torch.cat(states[:-1]))\n",
    "    replay_buffer.data_log(\"next_states\", torch.cat(states[1:]))\n",
    "    replay_buffer.data_log(\"rewards\", torch.cat(rewards))\n",
    "    replay_buffer.data_log(\"actions\", torch.cat(actions))\n",
    "    replay_buffer.data_log(\"masks\", torch.cat(masks))\n",
    "\n",
    "    if replay_buffer.buffer_full():\n",
    "        dqn_update()\n",
    "\n",
    "        if rollouts % 2 == 0:\n",
    "            new_lr = max(1e-4, ((max_steps - step)/max_steps) * lr)\n",
    "            epsilon = max(0.2, ((max_steps - step)/max_steps) * initial_epsilon)\n",
    "            \n",
    "            optimizer.param_groups[0][\"lr\"] = new_lr\n",
    "\n",
    "            score_logger.append(np.mean([test_agent() for _ in range(10)]))\n",
    "            clear_output(True)\n",
    "            plt.plot(score_logger)\n",
    "            plt.show()\n",
    "    rollouts +=1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(score_logger)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
